{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b66876-dec4-4aab-8341-94e7fd4987e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anomaly Detection-1 assignment\n",
    "\n",
    "\"\"\"Q1. What is anomaly detection and what is its purpose?\"\"\"\n",
    "\n",
    "Ans: Anomaly detection is a technique used in data analysis and machine learning to identify unusual patterns, outliers,\n",
    "or data points that deviate significantly from the expected or normal behavior within a dataset. Its primary purpose is\n",
    "to highlight instances that are rare, suspicious, or potentially indicative of errors, fraud, or other unusual events.\n",
    "Anomalies can take various forms, including:\n",
    "\n",
    "Point Anomalies: These are single data points that are significantly different from the majority of the data. \n",
    "For example, detecting a fraudulent credit card transaction or a sensor reading indicating a malfunction.\n",
    "\n",
    "Contextual Anomalies: In this case, the anomaly is context-dependent. It considers the surrounding data to determine if\n",
    "a data point is anomalous. For example, a sudden drop in website traffic during a holiday season could be considered an\n",
    "anomaly.\n",
    "\n",
    "Collective Anomalies: Collective anomalies involve a group of data points that together exhibit anomalous behavior. \n",
    "Detecting outbreaks of a disease in a specific geographic area or identifying a coordinated cyberattack on a network \n",
    "are examples of collective anomalies.\n",
    "\n",
    "The purposes and applications of anomaly detection are diverse and include:\n",
    "\n",
    "Fraud Detection: Detecting fraudulent activities in financial transactions, such as credit card fraud, insurance fraud,\n",
    "or identity theft.\n",
    "\n",
    "Network Security: Identifying unusual patterns in network traffic that may indicate cyberattacks, intrusion attempts,\n",
    "or malware infections.\n",
    "\n",
    "Industrial Equipment Monitoring: Monitoring sensor data from machinery and equipment to detect anomalies that may\n",
    "indicate equipment malfunction or maintenance needs.\n",
    "\n",
    "Healthcare: Identifying unusual patient data or vital signs that could indicate medical conditions or anomalies in\n",
    "healthcare data.\n",
    "\n",
    "Quality Control: In manufacturing, it can be used to identify defective products on the production line.\n",
    "\n",
    "Environmental Monitoring: Detecting abnormal environmental conditions such as pollution spikes, earthquakes, or unusual\n",
    "weather patterns.\n",
    "\n",
    "User Behavior Analysis: Analyzing user behavior on websites or apps to detect unusual patterns that may indicate\n",
    "fraudulent or malicious activity.\n",
    "\n",
    "Anomaly Detection in Time Series Data: Detecting irregularities in time-series data, such as stock prices, energy\n",
    "consumption, or weather data, to identify unusual trends or events.\n",
    "\n",
    "Anomaly detection algorithms can vary in complexity, from simple statistical methods like Z-score or percentile-based\n",
    "approaches to more advanced techniques like machine learning-based models (e.g., isolation forests, one-class SVMs,\n",
    "autoencoders) that can handle high-dimensional data and learn complex patterns. The choice of the method depends on the\n",
    "specific application and the nature of the data being analyzed.\n",
    "\n",
    "\"\"\"Q2. What are the key challenges in anomaly detection?\"\"\"\n",
    "\n",
    "Ans: Anomaly detection is a valuable technique, but it also comes with several key challenges that practitioners must address\n",
    "to achieve accurate and reliable results. Some of the key challenges in anomaly detection include:\n",
    "\n",
    "Imbalanced Data: In many real-world applications, anomalies are rare compared to normal data. This class imbalance can make it\n",
    "difficult for models to learn the characteristics of anomalies effectively. Special techniques like oversampling, undersampling,\n",
    "or using different evaluation metrics (e.g., area under the precision-recall curve) are often necessary to handle imbalanced\n",
    "data.\n",
    "\n",
    "Noisy Data: Data can contain noise, which is random or irrelevant information that can confuse anomaly detection algorithms.\n",
    "Cleaning and preprocessing the data are essential steps to reduce noise.\n",
    "\n",
    "High-Dimensional Data: Anomaly detection can become more challenging as the dimensionality of the data increases. \n",
    "High-dimensional spaces can make it difficult to define what constitutes an anomaly and can lead to the curse of dimensionality.\n",
    "Dimensionality reduction techniques may be needed to address this challenge.\n",
    "\n",
    "Dynamic Environments: In some applications, the definition of what is considered normal can change over time. Anomaly detection\n",
    "models must be adaptable to evolving patterns and concepts, which requires continuous monitoring and model retraining.\n",
    "\n",
    "Interpretable Anomalies: Identifying anomalies is not enough; understanding why a data point is anomalous is often critical.\n",
    "Interpretable anomaly detection methods are essential for explaining and addressing anomalies effectively.\n",
    "\n",
    "Scalability: Some applications, such as network traffic monitoring or sensor data analysis, involve large volumes of data that\n",
    "need to be processed in real-time. Building scalable anomaly detection systems can be a challenge.\n",
    "\n",
    "Choosing the Right Algorithm: There is no one-size-fits-all algorithm for anomaly detection. Selecting the appropriate\n",
    "algorithm for a specific dataset and problem can be a complex task that requires domain expertise.\n",
    "\n",
    "Labeling Anomalies: In many cases, obtaining labeled data for anomalies can be difficult and expensive. Semi-supervised or\n",
    "unsupervised approaches are often used to deal with the lack of labeled anomalies.\n",
    "\n",
    "Evaluating Performance: Evaluating the performance of an anomaly detection model can be tricky, especially when anomalies are\n",
    "rare. Traditional metrics like accuracy may not be suitable, and alternative metrics like precision, recall, F1-score, or area\n",
    "under the receiver operating characteristic curve (AUC-ROC) are often used.\n",
    "\n",
    "Anomaly Interpretation: After detecting an anomaly, it's crucial to understand its significance and potential impact. Domain\n",
    "knowledge and context are essential for interpreting anomalies correctly.\n",
    "\n",
    "Privacy Concerns: In applications involving sensitive data (e.g., healthcare or financial data), privacy concerns may limit\n",
    "the extent to which data can be shared or analyzed for anomaly detection.\n",
    "\n",
    "Addressing these challenges often requires a combination of domain expertise, data preprocessing, feature engineering, the use\n",
    "of appropriate algorithms, and ongoing monitoring and adaptation of anomaly detection systems. Additionally, the choice of\n",
    "technique and approach should align with the specific characteristics and requirements of the problem at hand.\n",
    "\n",
    "\"\"\"Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?\"\"\"\n",
    "\n",
    "Ans: Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches to identifying anomalies\n",
    "within a dataset. They differ primarily in terms of the availability of labeled data during the training process and the way\n",
    "anomalies are detected:\n",
    "\n",
    "Supervised Anomaly Detection:\n",
    "\n",
    "Labeled Data: In supervised anomaly detection, you have a dataset in which each data point is labeled as either \"normal\" or\n",
    "\"anomalous.\" This means you know in advance which data points are anomalies and which are not. Labeling often requires domain\n",
    "knowledge or manual inspection.\n",
    "Training Process: You train a supervised model, typically a classification algorithm (e.g., logistic regression, decision tree,\n",
    "support vector machine), using the labeleddata. The model learns to distinguish between normal and anomalous instances based on\n",
    "the provided labels.\n",
    "\n",
    "Detection: During the testing or deployment phase, the trained model is used to predict whether new, unseen data points are\n",
    "normal or anomalous. The model assigns a label (normal or anomalous) to each data point based on its learned patterns.\n",
    "Advantages: Supervised anomaly detection tends to be highly accurate when sufficient labeled data is available. It can provide\n",
    "detailed information about why a particular data point is considered an anomaly.\n",
    "\n",
    "Unsupervised Anomaly Detection:\n",
    "Lack of Labeled Data: Unsupervised anomaly detection, as the name suggests, does not rely on labeled data. It operates under\n",
    "the assumption that anomalies are rare and different from normal data points.\n",
    "Training Process: Unsupervised methods aim to learn the inherent structure or characteristics of the normal data distribution\n",
    "without specific knowledge of anomalies. Common techniques include clustering, density estimation, and dimensionality reduction\n",
    "(e.g., k-means, Gaussian Mixture Models, Isolation Forest, Autoencoders).\n",
    "\n",
    "Detection: Once the model is trained on normal data, it can identify anomalies by identifying data points that deviate \n",
    "significantly from the learned normal behavior. This is typically done by calculating a distance, score, or likelihood measure.\n",
    "\n",
    "Advantages: Unsupervised anomaly detection is useful when labeled data is scarce or expensive to obtain. It can identify novel\n",
    "or previously unseen anomalies. However, it may not provide detailed explanations for why a data point is considered an anomaly.\n",
    "In summary, the key differences between unsupervised and supervised anomaly detection lie in the availability of labeled data \n",
    "and the approach to modeling anomalies. Unsupervised methods rely on learning the normal data distribution without labeled\n",
    "anomalies, making them more flexible but potentially less precise. In contrast, supervised methods require labeled data and\n",
    "provide more precise anomaly detection but require the effort of labeling data points as anomalies, which may not always be\n",
    "feasible or practical. The choice between these approaches depends on the specific requirements and constraints of the problem\n",
    "at hand.\n",
    "\n",
    "\"\"\"Q4. What are the main categories of anomaly detection algorithms?\"\"\"\n",
    "\n",
    "Ans: Anomaly detection algorithms can be categorized into several main categories based on their underlying techniques and \n",
    "approaches. These categories include:\n",
    "\n",
    "Statistical Methods:\n",
    "\n",
    "Z-Score: This method measures how many standard deviations a data point is from the mean. Data points with high z-scores are\n",
    "considered anomalies.\n",
    "Percentile Ranks: It identifies anomalies based on their position in the data distribution, such as data points in the tails\n",
    "(e.g., values below the 5th percentile or above the 95th percentile).\n",
    "\n",
    "Machine Learning-Based Methods:\n",
    "Supervised Learning: While not typically considered \"anomaly detection,\" supervised algorithms like logistic regression, \n",
    "decision trees, and support vector machines can be used for anomaly detection when trained on labeled data.\n",
    "Unsupervised Learning: These methods include clustering algorithms (e.g., k-means), density estimation techniques\n",
    "(e.g., Gaussian Mixture Models), and dimensionality reduction approaches (e.g., Principal Component Analysis) to identify \n",
    "anomalies based on deviations from learned normal patterns.\n",
    "\n",
    "Semi-Supervised Learning: A combination of supervised and unsupervised approaches, where a model is trained on normal data\n",
    "but may use some labeled anomalies for fine-tuning.\n",
    "\n",
    "Distance-Based Methods:\n",
    "Euclidean Distance: Measures the distance between data points in a multi-dimensional space. Anomalies are often data points\n",
    "that are farthest from their nearest neighbors.\n",
    "\n",
    "Mahalanobis Distance: Accounts for correlations between variables and is useful for high-dimensional data.\n",
    "Cosine Similarity: Measures the cosine of the angle between vectors. It's commonly used for text or document similarity,\n",
    "but it can also be adapted for anomaly detection.\n",
    "\n",
    "Density-Based Methods:\n",
    "Local Outlier Factor (LOF): Measures the density of data points compared to their neighbors. Anomalies have significantly\n",
    "different local densities.\n",
    "\n",
    "Isolation Forest: Constructs an ensemble of decision trees to isolate anomalies efficiently by taking fewer splits in the tree\n",
    "to separate them.\n",
    "\n",
    "One-Class SVM (Support Vector Machine): Learns a hyperplane that separates the majority of the data from the rest, which are\n",
    "considered anomalies.\n",
    "\n",
    "Clustering-Based Methods:\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Clusters data points based on density, considering\n",
    "isolated points as anomalies.\n",
    "\n",
    "K-Means-Based Approaches: Use clustering to identify anomalies as data points that do not belong to any cluster or belong to\n",
    "small clusters.\n",
    "\n",
    "Deep Learning-Based Methods:\n",
    "Autoencoders: Neural networks designed to learn efficient representations of data. Anomalies can be detected when the\n",
    "reconstruction error is high.\n",
    "\n",
    "Variational Autoencoders (VAEs): A type of autoencoder that models data using probabilistic techniques, making them suitable\n",
    "for anomaly detection with uncertainty estimates.\n",
    "\n",
    "Time Series Anomaly Detection:\n",
    "ARIMA-Based Approaches: Use Autoregressive Integrated Moving Average models for time series anomaly detection.\n",
    "\n",
    "Prophet: A forecasting tool that can be adapted to detect anomalies in time series data.\n",
    "Recurrent Neural Networks (RNNs) and LSTM Networks: Deep learning models for sequential data that can capture temporal\n",
    "patterns for anomaly detection in time series.\n",
    "\n",
    "These categories encompass a wide range of techniques, and the choice of which method to use depends on the specific\n",
    "characteristics of the data, the nature of the anomalies, the availability of labeled data, and the desired level of\n",
    "interpretability and computational efficiency. In practice, a combination of methods or ensemble techniques may also be\n",
    "employed to improve anomaly detection performance.\n",
    "\n",
    "\"\"\"Q5. What are the main assumptions made by distance-based anomaly detection methods?\"\"\"\n",
    "\n",
    "Ans:Distance-based anomaly detection methods rely on certain assumptions about the data and the nature of anomalies. These \n",
    "assumptions are important to understand because they influence the effectiveness and applicability of these methods. The main\n",
    "assumptions made by distance-based anomaly detection methods include:\n",
    "\n",
    "Assumption of Normality:\n",
    "\n",
    "Distance-based methods often assume that normal data points are distributed in a dense and compact manner, forming clusters or\n",
    "following a particular distribution (e.g., Gaussian distribution).\n",
    "Anomalies are expected to be located far away from these normal data clusters, making them distinct outliers.\n",
    "Euclidean Space Assumption:\n",
    "\n",
    "Many distance-based methods, particularly those using Euclidean distance, assume that the data can be represented in a\n",
    "Euclidean space with a fixed number of dimensions.\n",
    "The distance metric used (e.g., Euclidean distance, Mahalanobis distance) is designed for such spaces and may not work well \n",
    "for data with complex structures or high dimensionality.\n",
    "Single-Cluster Assumption:\n",
    "\n",
    "Some distance-based methods assume that normal data points belong to a single large cluster or distribution. Anomalies are \n",
    "then identified as data points that are distant from this main cluster.\n",
    "This assumption may not hold when the data consists of multiple clusters or has a complex, multi-modal distribution.\n",
    "Symmetric Distance Assumption:\n",
    "\n",
    "Many distance metrics assume that the distance between two points is symmetric, meaning the distance from point A to point B\n",
    "is the same as the distance from point B to point A.\n",
    "While this assumption is true for Euclidean distance, it may not hold for all types of data or distance metrics.\n",
    "Fixed Threshold Assumption:\n",
    "\n",
    "Distance-based anomaly detection methods often require setting a fixed threshold beyond which data points are considered \n",
    "anomalies.\n",
    "Determining an appropriate threshold can be challenging and may vary depending on the application. It may also result in false\n",
    "positives or false negatives if not chosen carefully.\n",
    "Outlier Sensitivity Assumption:\n",
    "\n",
    "These methods are often sensitive to the presence of outliers in the normal data. Outliers can affect the location of the \n",
    "threshold and the effectiveness of anomaly detection.\n",
    "Robustness to outliers may require preprocessing steps or alternative distance metrics.\n",
    "Metric Assumption:\n",
    "\n",
    "Different distance metrics may yield different results, and the choice of metric can impact the performance of distance-based\n",
    "methods.\n",
    "The metric chosen should align with the characteristics of the data and the desired sensitivity to anomalies.\n",
    "Homogeneity Assumption:\n",
    "\n",
    "Distance-based methods may assume that normal data points have similar properties or characteristics.\n",
    "Anomalies are expected to have significantly different properties from normal data points.\n",
    "It's important to note that these assumptions may not always hold in real-world scenarios, and the effectiveness of \n",
    "distance-based methods can be influenced by the degree to which these assumptions are violated. Therefore, careful \n",
    "consideration of the data and the problem at hand is essential when choosing and applying distance-based anomaly detection\n",
    "techniques. Additionally, combining distance-based methods with other approaches, such as density-based or machine\n",
    "learning-based methods, can enhance anomaly detection performance in cases where these assumptions may not be met.\n",
    "\n",
    "\"\"\"Q6. How does the LOF algorithm compute anomaly scores?\"\"\"\n",
    "\n",
    "Ans: The Local Outlier Factor (LOF) algorithm computes anomaly scores for data points in a dataset by measuring the local\n",
    "deviation of a data point from its neighbors. LOF is a density-based anomaly detection algorithm that takes into account the\n",
    "density of data points in the vicinity of each point to identify anomalies. Here's how LOF computes anomaly scores:\n",
    "\n",
    "k-Nearest Neighbors (k-NN) Selection:\n",
    "LOF starts by defining a neighborhood around each data point. It does this by finding the k-nearest neighbors of the data\n",
    "point, where \"k\" is a user-defined parameter.\n",
    "These k-nearest neighbors are the data points that are closest to the target data point in terms of some distance metric \n",
    "(usually Euclidean distance).\n",
    "Reachability Distance Calculation:\n",
    "\n",
    "For each data point, LOF calculates the reachability distance of that point with respect to its k-nearest neighbors.\n",
    "The reachability distance of a data point A with respect to another data point B is defined as the maximum of the distance\n",
    "between A and B and the distance between B and its k-th nearest neighbor (i.e., the neighbor farthest from B among the\n",
    "k-nearest neighbors).\n",
    "Local Reachability Density Calculation:\n",
    "\n",
    "LOF calculates the local reachability density (LRD) for each data point. The LRD of a data point is the inverse of the average\n",
    "reachability distance of that point with respect to its k-nearest neighbors.\n",
    "\n",
    "LRD(A) = 1 / (Σ reachability distance(A, B) for all B in the k-nearest neighbors of A)\n",
    "Local Outlier Factor (LOF) Calculation:\n",
    "\n",
    "Finally, LOF computes the Local Outlier Factor for each data point. The LOF of a data point A measures how different the LRD\n",
    "of A is from the LRDs of its k-nearest neighbors.\n",
    "LOF(A) = (Σ LRD(B) for all B in the k-nearest neighbors of A) / (k * LRD(A))\n",
    "A high LOF value indicates that the data point A has a significantly lower local density compared to its neighbors, suggesting\n",
    "it is an anomaly.\n",
    "\n",
    "Thresholding and Ranking:\n",
    "After computing LOF values for all data points, a threshold is typically set to classify data points as anomalies or normal.\n",
    "Data points with LOF values significantly greater than the threshold are considered anomalies, while those below the threshold\n",
    "are considered normal.\n",
    "\n",
    "The data points can also be ranked based on their LOF values, allowing you to focus on the most anomalous data points.\n",
    "In summary, the LOF algorithm computes anomaly scores by assessing how the local density of a data point compares to the local\n",
    "densities of its neighbors. Points with significantly lower local densities are considered anomalies, as they deviate from the\n",
    "expected density patterns in the dataset. LOF is effective at identifying anomalies in datasets with varying local densities \n",
    "and complex structures.\n",
    "\n",
    "\"\"\"Q7. What are the key parameters of the Isolation Forest algorithm?\"\"\"\n",
    "\n",
    "Ans: The Isolation Forest algorithm is a machine learning-based anomaly detection method that works by isolating anomalies in\n",
    "a dataset. It is known for its efficiency and effectiveness in identifying anomalies, especially in high-dimensional datasets.\n",
    "The key parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "n_estimators (default: 100):\n",
    "This parameter specifies the number of isolation trees to build in the forest. Increasing the number of trees can improve the\n",
    "algorithm's accuracy but may also increase computation time.\n",
    "\n",
    "max_samples (default: \"auto\"):\n",
    "It determines the number of data points to sample from the dataset when constructing each isolation tree. The default value\n",
    "\"auto\" typically means that it samples min(256, n_samples) data points. You can set it to an integer value to control the\n",
    "number of samples used.\n",
    "\n",
    "contamination (default: \"auto\"):\n",
    "This parameter sets the expected fraction of anomalies in the dataset. It is used to determine the threshold for classifying\n",
    "data points as anomalies. The \"auto\" value is typically set to 0.1 (10%) but can be adjusted based on your prior knowledge\n",
    "about the dataset.\n",
    "\n",
    "max_features (default: 1.0):\n",
    "It controls the maximum number of features to consider when splitting a node in an isolation tree. A value of 1.0 means that\n",
    "all features are considered, while a value less than 1.0 selects a random subset of features for each split. Limiting features\n",
    "can speed up training and may improve the algorithm's performance, especially in high-dimensional datasets.\n",
    "\n",
    "bootstrap (default: False):\n",
    "When set to True, it enables bootstrapping, which means that the dataset is sampled with replacement when constructing each \n",
    "isolation tree. Bootstrapping can introduce randomness and improve the diversity of the trees in the forest.\n",
    "\n",
    "random_state (default: None):\n",
    "It sets the random seed for reproducibility. If you want to obtain consistent results across multiple runs, you can specify a\n",
    "fixed random_state value.\n",
    "\n",
    "These parameters allow you to control various aspects of the Isolation Forest algorithm's behavior, such as the number of\n",
    "trees in the forest, the sampling strategy, and the contamination threshold for classifying anomalies. Depending on the\n",
    "characteristics of your dataset and the specific anomaly detection task, you may need to tune these parameters to achieve\n",
    "optimal results. Cross-validation or grid search can be useful techniques for parameter tuning. \n",
    "\n",
    "\"\"\"Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?\"\"\"\n",
    "\n",
    "Ans: To compute the anomaly score of a data point using k-Nearest Neighbors (KNN) with K=10, we need to consider the density of\n",
    "data points within the specified radius (0.5) around the data point. The anomaly score depends on how many of the K nearest\n",
    "neighbors (K=10) fall within this radius and share the same class. If the data point has only 2 neighbors of the same class \n",
    "within the radius, we can calculate its anomaly score as follows:\n",
    "\n",
    "Anomaly Score = (Number of Same-Class Neighbors within Radius) / K\n",
    "\n",
    "In this case:\n",
    "\n",
    "Number of Same-Class Neighbors within Radius = 2 (as specified)\n",
    "K (the total number of nearest neighbors considered) = 10 (as specified)\n",
    "Now, plug these values into the formula:\n",
    "\n",
    "Anomaly Score = 2 / 10 = 0.2\n",
    "\n",
    "So, the anomaly score for the data point in question, based on having 2 neighbors of the same class within a radius of 0.5\n",
    "with K=10, is 0.2. This score indicates that the data point is not considered a strong anomaly since it has some neighbors of\n",
    "the same class within the specified radius. Anomalies typically have lower anomaly scores, closer to 0, when they are distinct\n",
    "from their neighbors.\n",
    "\n",
    "\"\"\"Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a\n",
    "data point that has an average path length of 5.0 compared to the average path length of the trees?\"\"\"\n",
    "\n",
    "Ans: In the Isolation Forest algorithm, the anomaly score for a data point is calculated based on its average path length\n",
    "compared to the average path length of the trees in the forest. The average path length is a measure of how deep into the trees\n",
    "a data point descends before it is isolated. A shorter path length indicates that the data point is easier to isolate and may\n",
    "be more likely to be an anomaly.\n",
    "\n",
    "The anomaly score calculation for a data point is defined as:\n",
    "\n",
    "Anomaly Score = 2^(-average_path_length / c(n))\n",
    "\n",
    "Where:\n",
    "\n",
    "average_path_length is the average path length of the data point in the forest.\n",
    "c(n) is a constant that depends on the number of data points in the dataset (n).\n",
    "The constant c(n) is given by:\n",
    "\n",
    "c(n) = 2 * (log(n - 1) + 0.5772156649) - (2 * (n - 1) / n)\n",
    "\n",
    "In this case, you have:\n",
    "\n",
    "Number of trees (n_trees) = 100 (as specified)\n",
    "Number of data points (n) = 3000 (as specified)\n",
    "Average path length for the data point (average_path_length) = 5.0 (as specified)\n",
    "First, calculate the constant c(n):\n",
    "\n",
    "c(3000) = 2 * (log(3000 - 1) + 0.5772156649) - (2 * (3000 - 1) / 3000)\n",
    "\n",
    "Next, plug the values into the anomaly score formula:\n",
    "\n",
    "Anomaly Score = 2^(-5.0 / c(3000))\n",
    "\n",
    "Calculate the value of c(3000) and then compute the anomaly score using the formula above. The resulting anomaly score will \n",
    "indicate how different the data point's average path length is from the expected average path length of a typical data point\n",
    "in the forest. A lower anomaly score suggests that the data point is more likely to be an anomaly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
